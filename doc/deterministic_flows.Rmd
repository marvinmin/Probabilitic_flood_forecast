---
output:
  github_document:
    toc: true
    toc_depth: 1
    pandoc_args: --webtex
---

```{r packages, include=FALSE}
library(tidyverse)
library(quantreg)
library(lubridate)
library(ggpubr)
```

## Purpose
The purpose of this notebook is to produce exact probability distributions of flow for `n` time steps into the future using a model with quantile outputs. The “Functions” section (as the name suggests) will define the functions that will be used for the demonstration in the “Modelling” and “Visualization” sections. Explanation of usage is provided for all functions. 


## Data Cleaning

In the cell below, we are loading the dataset, selecting our gauge of interest, splitting data into training and testing sets.

```{r}
# User Specificed Data
gauge <- '05DF008_flow_m3s-1'
train_test_ratio <- 0.83


# Reading data file and selecting desired guage/cleaning
# Make sure that the names of the columns match
river_flow <- read_csv("../data/station_flowrate_R.csv",
                       col_types = cols(.default = "d", time = "D")) %>% 
  select('time', gauge) %>% 
  drop_na() %>% 
  rename('date' = time, 'flow' = gauge) %>% 
  filter(leap_year(date) == FALSE) %>% 
  filter(flow != 0) %>% 
  mutate(Year = year(date), Day = yday(date)) %>%
  select(Year, Day, flow)

# Train and test data split
train_data <- river_flow[1:ceiling(length(river_flow$flow)*train_test_ratio), ]
test_data <- river_flow[-(1:ceiling(length(river_flow$flow)*train_test_ratio)), ]
```


## Functions

Below is a function to convert quanitle results to actual sample points. For example, after fitting a quantile regression model and using the `predict` method to generate points corresponding to different quantiles, I use these quantiles in order to generate points that, in a perfect world, would appear as a “set” to preserve our respective quantile distribution. For example, given a coin, we would have one head for every tail, so that the overall probability is conserved.

We will do this by taking the mean between quantile points. For example, if the 1st quantile has a flow of 5 and the 2nd quantile has a flow of 10, we will take our point to be 7.5. However, we run into edge cases here. For instance, there has to be a point lower than the 1st quantile and higher than the 99th qauntile. To do this, we will assume that the distance away from the lowest quantile be the same as the distance from the 1st quantile to its nearest neighbouring sample point. So for the example above, we will use a flow of 2.5. The same applies to the other end of the spectrum for the highest sample point.

The above is done to help preserve the distribution of the quantile regression.

```{r quantile to points}
#' @param quantiles series the quantile outputs after using predict on a `rq` model
#'
#' @return list of all the points presenting that quantile distribution
quantiles_to_points <- function(quantiles) {
  
  # Creating a empty vector to store all the points that will represent the quantile distribution
  my_points <- vector('double', length(quantiles))
  
  for (n in 1:(length(quantiles) + 1)) {
    
    if (n == 1) {
      # This deals with the first quantile point
      quantile_flow <- quantiles[1] - 0.5*(quantiles[2] - quantiles[1])
      my_points[n] <- quantile_flow
      
    } else if (n == (length(quantiles) + 1)) {
      # This deals with the last quantile point
      quantile_flow <- quantiles[n-1] + 0.5*(quantiles[n-1] - quantiles[n-2])
      my_points[n] <- quantile_flow
      
    } else {
      # Quantiles in the middle
      quantile_flow <- (quantiles[n] + quantiles[n-1])/2
      my_points[n] <- quantile_flow
      
    }
  }
  return(my_points)
}
```

When simulating for distributions multiple days ahead, we run into the issue of exploding computation power needed to full characterize the distribution. This step acts as a buffer to cap the amount of extra computation power needed when simulating a large number of days ahead.

For example, say we are simulating for the first day after our train data, and our quantile regressor estimates 99 quantiles (100 data points). Now, for the first day of the test set, our input is fixed, the lag1, lag2 ... are fixed values as we know them. However, for our prediction day, our quantile regressor provides us with 100 points of equal probabilty. Ok, so what's is the big deal? There will only be 100 different input data "sets" when predicting day 2 right? While that may be true, we qucikly realize that each one of the 100 input sets produces 100 sample points itself, yielding us a total of 100^2 data points of equal probability for day 2. If we continue like this without doing something about it, the computation time for each day forward will only grow exponentially.

As a result, I have created a function that shrinks the number of recorded points each day to be a fixed resolution. For example, for the 100^2 data points created when predicting day 2, I will bin the points into 100 bins and then take the mean of each of the bins to be the final values used for that day.

Feel free the change the resolution as desired.

```{r condensing sampled points}
#' @param temp_data list of all data points generated from the different input data sets used
#' @param resolution int how many data points are generated from a single input. For example, if our model generates quantiels 1:99/100, then the resolution would be 100, as it is the number of points that can be generated to represent the 99 quantiles.
#'
#' @return list a condensed list of all of the data points we have generated
condense_points <- function(temp_data, resolution) {
  # Seeing how big each of the bins should be
  bin_size <- length(temp_data)/resolution
  my_sorted <- sort(temp_data)
  condensed_list <- vector('double', resolution)
  
  for (n in 1:resolution) {
    # taking the mean of each of the bins
    condensed_list[n] <- mean(my_sorted[((n - 1)*bin_size + 1):(n*bin_size)])
  }
  
  return(condensed_list)
}

```

The last problem that we encounter is meshing together points from different days. Again, we go back to our example. When predicting the first day of our test data, all of the input data are known, and therefore we only derive one distribution of 100 points. When predicting day 2, each one of the data points for day 1 will represent its own unique input data set, thus creating 100 unique input sets. Likewise, for day 2 we derive 100 points (using the function above). However, we realize that, for day 3, we now have 100^2 unique input sets. The number of input data points will grow exponentially up until the maximum lag that we are interested in (e.g. we will have a maximum of 100^3 points when using 3 lags). 

In short, the heaviest part of the computation needed will be proportional to $resoution^{lags}$.

To ensure that possible combinations are covered, the following function generates a DataFrame that covers all possibilities.

```{r}
#' @param current_day int the number of days that we have already simulated
#' @param resolution int how many data points are generated from a single input. For example, if our model generates quantiels 1:99/100, then the resolution would be 100, as it is the number of points that can be generated to represent the 99 quantiles.
#' @param max_lags int the maximum number of autoregressive lags that our model looks at
#' @param input_data data.frame Data Frame of unique input sets we currently have
#' @param sample_storage data.frame Data Frame containing the unique sample points for each individual day
#'
#' @return list a condensed list of all of the data points we have generated
create_input_data <- function(current_day, resolution, max_lags, input_data, sample_storage) {
  
  # Seeing how many rows are required to store all possible combinations
  if (current_day <= max_lags) {
    num_rows <- resolution**current_day
  } else {
    num_rows <- resolution**max_lags
  }
  
  # Creating empty DataFrame to store input sets
  data_storage <- data.frame(matrix(ncol=max_lags, nrow=num_rows))
  
  if (current_day < max_lags) {
    # Copying data over that remains the same
    data_storage[, (current_day + 1):max_lags] <- input_data[, current_day:(max_lags - 1)]
    # Data that needs to be permutated
    data_storage[, 1:current_day] <- expand.grid(sample_storage[, current_day:1])
  } else {
    data_storage[, 1:max_lags] <- expand.grid(sample_storage[, current_day:(current_day-max_lags+1)])
  }
  
  colnames(data_storage) <- c('lag1', 'lag2', 'lag3')
  
  return(data_storage)
}
```


## Modelling

Here we create the quantile regression model that will be used for predicting future data.

```{r creating baseline model}
# User specified info for loess function
my_span <- 0.1
my_tau <- seq(from = 2, to =98 , by = 2)/100


# Training initial model
qr_data <- data.frame('flow' = train_data$flow,
                      'lag1' = lag(train_data$flow, n = 1),
                      'lag2' = lag(train_data$flow, n = 2),
                      'lag3' = lag(train_data$flow, n = 3))

my_model <- rq(flow ~ lag1 + lag2 + lag3, tau = my_tau, data = qr_data)

```


Here we will predict the distributions of flows for any number of given days ahead. Note that the computational difficulty increases exponentially as the number of days we are trying to predict approaches the maximum number of lags of the model. After that, since the input data set has already reached maximum size, computation time will scale linearly with the number of days.



``` {r predicting n days ahead}
# User specified info for prediction
current_day <- 0
days_forward <- 6
input_data <- data.frame()
sample_storage <- data.frame(matrix(ncol=days_forward, nrow=(length(my_tau) + 1)))
colnames(sample_storage) <- c(1:days_forward)
resolution <- length(my_tau) + 1


# Prediction
while (days_forward > 0) {
  # This is only triggered on the first iteration, as input_data is not yet initialized
  if (length(input_data) == 0) {
    
    # Taking the last day of the train set as continuation
    input_sample <- data.frame('lag1' = tail(qr_data$flow, n = 1),
                               'lag2' = tail(qr_data$lag1, n = 1),
                               'lag3' = tail(qr_data$lag2, n = 1))
    
    # Quantile prediction
    quantile_predictions <- predict(my_model, newdata = input_sample)
    # Conversion to points
    my_points <- quantiles_to_points(quantile_predictions)
    # Updating day
    current_day <- current_day + 1
    # Adding the final sample points
    sample_storage[, current_day] <- my_points
    # Updating remaining number of days needed to predict
    days_forward <- days_forward - 1
    # Creating the input data sets
    input_data <- create_input_data(current_day = current_day,
                                    resolution = resolution,
                                    max_lags = 3,
                                    input_data = input_sample,
                                    sample_storage = sample_storage)
    # How many times we need to iterate
    n_iter <- nrow(input_data)
    # Temporary storage for the all the sample points generated from the new input sets
    temp_data <- vector('double', resolution*nrow(input_data))
  
  # This is triggered as we iterate through the input sets
  } else if (n_iter > 0) {
    
    input_sample <- input_data[n_iter, ]
    
    #attempt(predict(my_model, newdata = input_sample), msg = n_iter)
    quantile_predictions <- predict(my_model, newdata = input_sample)
    
    my_points <- quantiles_to_points(quantile_predictions)
    # Adding the points to temporary storage
    temp_data[((n_iter-1)*length(my_points) + 1):(n_iter*length(my_points))] <- my_points
    n_iter <- n_iter - 1
  
  # This is triggered when we are finished all iterations for a given day  
  } else if (n_iter == 0) {
    
    current_day <- current_day + 1
    # Adding the final sample points after condensing
    sample_storage[, current_day] <- condense_points(temp_data = temp_data,
                                                     resolution = resolution)
    days_forward <- days_forward - 1
    
    if (days_forward > 0) {
      
      
      input_data <- create_input_data(current_day = current_day,
                                      resolution = resolution,
                                      max_lags = 3,
                                      input_data = input_data,
                                      sample_storage = sample_storage)
      
      n_iter <- nrow(input_data)
      print(paste('The number of test set iterations for step', current_day, 'is', n_iter, sep = ' '))
      temp_data <- vector('double', resolution*nrow(input_data))
    }
  }
}
```

Cleaning results for presentation

```{r Beautify results}
final_storage <- data.frame(sample_storage)

for (n in 1:ncol(final_storage)) {
  colnames(final_storage)[n] <- paste('Day', n, sep = ' ')
}

final_storage

```

Here are the actual flows, with the first row corresponding to `Day 1` ...etc.

```{r actual results}
head(test_data)
```

# Visualization

Here we will plot the density for each day to obtain the probability distribution for each day. I have not automated the plotting aspect.

```{r Viz, fig.height = 15, fig.width = 10}

plot1 <- ggplot(data = final_storage) +
  geom_density(aes(`Day 1`)) +
  geom_vline(xintercept = test_data$flow[1], color = 'red', size = 1) +
  ggtitle('Day 1 PDF (Actual in Red)') +
  xlim(min(final_storage), max(final_storage)) +
  theme(axis.title.x = element_blank())

plot2 <- ggplot(data = final_storage) +
  geom_density(aes(`Day 2`)) +
  geom_vline(xintercept = test_data$flow[2], color = 'red', size = 1) +
  ggtitle('Day 2 PDF (Actual in Red)') +
  xlim(min(final_storage), max(final_storage)) +
  theme(axis.title.x = element_blank())

plot3 <- ggplot(data = final_storage) +
  geom_density(aes(`Day 3`)) +
  geom_vline(xintercept = test_data$flow[3], color = 'red', size = 1) +
  ggtitle('Day 3 PDF (Actual in Red)') +
  xlim(min(final_storage), max(final_storage)) +
  theme(axis.title.x = element_blank())

plot4 <- ggplot(data = final_storage) +
  geom_density(aes(`Day 4`)) +
  geom_vline(xintercept = test_data$flow[4], color = 'red', size = 1) +
  ggtitle('Day 4 PDF (Actual in Red)') +
  xlim(min(final_storage), max(final_storage)) +
  theme(axis.title.x = element_blank())

plot5 <- ggplot(data = final_storage) +
  geom_density(aes(`Day 5`)) +
  geom_vline(xintercept = test_data$flow[5], color = 'red', size = 1) +
  ggtitle('Day 5 PDF (Actual in Red)') +
  xlim(min(final_storage), max(final_storage)) +
  theme(axis.title.x = element_blank())

plot6 <- ggplot(data = final_storage) +
  geom_density(aes(`Day 6`)) +
  geom_vline(xintercept = test_data$flow[6], color = 'red', size = 1) +
  ggtitle('Day 6 PDF (Actual in Red)') +
  xlim(min(final_storage), max(final_storage)) +
  xlab('Flow (m^3/s)')

ggarrange(plot1, plot2, plot3, plot4, plot5, plot6, ncol = 1)

```


